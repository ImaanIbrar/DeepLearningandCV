{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\RadarLab\\LandmarksClassificationusingCNNs\\landmark_images\\landmark_images\\train\n"
     ]
    }
   ],
   "source": [
    "image_path=\"D:\\RadarLab\\LandmarksClassificationusingCNNs\\landmark_images\\landmark_images\"\n",
    "train_datapath=os.path.join(image_path,\"train\")\n",
    "test_datapath=os.path.join(image_path,\"test\")\n",
    "print(train_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\RadarLab\\LandmarksClassificationusingCNNs\\landmark_images\\landmark_images\\train\\49.Temple_of_Olympian_Zeus\\7f41eadaeb629930.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classes=[]\n",
    "trainimages_path=[]\n",
    "testimages_path=[]\n",
    "\n",
    "for path in os.listdir(train_datapath):\n",
    "      classes.append(path)\n",
    "      class_full_path= os.path.join(train_datapath,path)\n",
    "      for i in os.listdir(class_full_path):\n",
    "          full_img_path=os.path.join(class_full_path,i)\n",
    "          trainimages_path.append(full_img_path)\n",
    "\n",
    "for path in os.listdir(test_datapath):\n",
    "    test_class_path = os.path.join(test_datapath, path)\n",
    "    for img in os.listdir(test_class_path):\n",
    "        testimages_path.append(os.path.join(test_class_path, img))\n",
    "\n",
    "print(trainimages_path[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_class = {i:j for i, j in enumerate(classes)}\n",
    "class_to_idx = {value:key for key,value in idx_to_class.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "class Custom_Data(Dataset):\n",
    "    \n",
    "    def __init__(self,image_path,transforms):\n",
    "        super().__init__()\n",
    "        self.image_path=image_path\n",
    "        self.transforms=transforms\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "    def __getitem__(self,index):\n",
    "        image_path=self.image_path[index]\n",
    "        image = Image.open(image_path)\n",
    "        #image.show()\n",
    "        label = image_path.split('\\\\')[-2]\n",
    "        label = class_to_idx[label]\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "tf=transforms.Compose([   \n",
    "         #transforms.ToPILImage(),\n",
    "         transforms.Resize(size=(224, 224)),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset=Custom_Data(trainimages_path,tf)\n",
    "test_dataset=Custom_Data(trainimages_path,tf)\n",
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= DataLoader(train_dataset,batch_size=100,shuffle=True)\n",
    "test_loader= DataLoader(test_dataset,batch_size=100,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "images, label = batch\n",
    "images.shape\n",
    "label.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,  # 1 for grey scale, 3 for RGB  , #n for n number of features maps\n",
    "                out_channels=16,    # number of feature maps\n",
    "                kernel_size=3,         # kernel size\n",
    "                stride=1,\n",
    "                padding=1, # (K-1)/2\n",
    "                #output size=   224(inputsise) - 3(kernel) + 2*0(padding) / 1(stride) +1  result 8 feature maps of 224x224\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),    # 224x224 reduced to 112x112\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32 ,3, 1, 1),    # 32 feature maps of 112x112\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # (generated feature map of size= (14-5+4) /2 )  After pooling = 56x56\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64 ,3 , 1, 1),    # 64 feature maps with size 56x56 produced by filter \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), \n",
    "        )\n",
    "        # fully connected layer, output 10 classes\n",
    "        self.out1 = nn.Linear(64*28*28 , 224)\n",
    "        self.dropout= nn.Dropout(0.3)\n",
    "        self.out2= nn.Linear(224 , 50)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x) # return (batch,channel,height,width)\n",
    "        x = torch.flatten(x,1)\n",
    "        output = self.out1(x)\n",
    "        output = self.dropout(output)\n",
    "        output = self.out2(output)\n",
    "        output = nn.Softmax(output)\n",
    "        \n",
    "        return output  # return x for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.01)\n",
    "num_epochs = 10\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Softmax(\n",
       "  dim=tensor([[-0.1022, -0.0784,  0.0134,  0.0643,  0.0960, -0.0945,  0.1736, -0.1053,\n",
       "           -0.0392, -0.1068, -0.0907, -0.0425,  0.0420,  0.0866,  0.0286,  0.0380,\n",
       "            0.0568, -0.0970,  0.0514,  0.0154, -0.0106,  0.1359,  0.0063, -0.0311,\n",
       "            0.0185,  0.0139, -0.0648, -0.0130, -0.1428,  0.0777, -0.0032,  0.0641,\n",
       "           -0.0511, -0.0997,  0.0753,  0.0554,  0.0322, -0.0335,  0.0042, -0.1518,\n",
       "            0.1379, -0.0762,  0.0751, -0.0433,  0.0580, -0.0333, -0.1061, -0.0585,\n",
       "           -0.0855, -0.0083]], grad_fn=<AddmmBackward0>)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_input = torch.randn(1, 3, 224, 224).to(device) \n",
    " # Adjust size as needed\n",
    "model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader,model,loss_fn,optimizer):\n",
    "    size=len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch,(X,y) in enumerate(dataloader):\n",
    "        X=X.to(device)\n",
    "        y=y.to(device)\n",
    "        #forward pass\n",
    "        y_pred=model(X)\n",
    "        #loss\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        #backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "                loss, current = loss.item(), batch * batch_size + len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.913688  [  100/ 4996]\n",
      "loss: 3.911410  [ 1100/ 4996]\n",
      "loss: 3.920201  [ 2100/ 4996]\n",
      "loss: 3.914086  [ 3100/ 4996]\n",
      "loss: 3.899322  [ 4100/ 4996]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.847996  [  100/ 4996]\n",
      "loss: 3.839329  [ 1100/ 4996]\n",
      "loss: 3.816904  [ 2100/ 4996]\n",
      "loss: 3.803485  [ 3100/ 4996]\n",
      "loss: 3.829708  [ 4100/ 4996]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.722653  [  100/ 4996]\n",
      "loss: 3.636260  [ 1100/ 4996]\n",
      "loss: 3.615500  [ 2100/ 4996]\n",
      "loss: 3.571392  [ 3100/ 4996]\n",
      "loss: 3.598412  [ 4100/ 4996]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.455754  [  100/ 4996]\n",
      "loss: 3.285309  [ 1100/ 4996]\n",
      "loss: 3.099061  [ 2100/ 4996]\n",
      "loss: 3.481343  [ 3100/ 4996]\n",
      "loss: 3.396529  [ 4100/ 4996]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.988665  [  100/ 4996]\n",
      "loss: 2.645732  [ 1100/ 4996]\n",
      "loss: 3.017979  [ 2100/ 4996]\n",
      "loss: 2.866925  [ 3100/ 4996]\n",
      "loss: 2.970355  [ 4100/ 4996]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 2.722767  [  100/ 4996]\n",
      "loss: 2.546150  [ 1100/ 4996]\n",
      "loss: 2.613650  [ 2100/ 4996]\n",
      "loss: 2.639232  [ 3100/ 4996]\n",
      "loss: 2.717690  [ 4100/ 4996]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 2.059113  [  100/ 4996]\n",
      "loss: 2.129964  [ 1100/ 4996]\n",
      "loss: 2.503998  [ 2100/ 4996]\n",
      "loss: 2.129980  [ 3100/ 4996]\n",
      "loss: 2.545919  [ 4100/ 4996]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 2.186019  [  100/ 4996]\n",
      "loss: 2.050359  [ 1100/ 4996]\n",
      "loss: 1.586546  [ 2100/ 4996]\n",
      "loss: 2.504825  [ 3100/ 4996]\n",
      "loss: 2.344369  [ 4100/ 4996]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.798806  [  100/ 4996]\n",
      "loss: 1.847019  [ 1100/ 4996]\n",
      "loss: 2.026876  [ 2100/ 4996]\n",
      "loss: 2.033153  [ 3100/ 4996]\n",
      "loss: 2.062182  [ 4100/ 4996]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.292618  [  100/ 4996]\n",
      "loss: 1.773205  [ 1100/ 4996]\n",
      "loss: 1.624510  [ 2100/ 4996]\n",
      "loss: 1.592836  [ 3100/ 4996]\n",
      "loss: 2.053031  [ 4100/ 4996]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for z in range(num_epochs):\n",
    "    print(f\"Epoch {z+1}\\n-------------------------------\")\n",
    "    train(train_loader, model, loss_fn, optimizer)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.78125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    # Test the model\n",
    "    best_loss=float(\"inf\")\n",
    "    test_loss=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            test_output = model(images)\n",
    "            test_loss +=loss_fn(test_output,labels).item()\n",
    "            pred_y = torch.max(test_output, 1)[1].data.squeeze()\n",
    "\n",
    "            if test_loss<best_loss:\n",
    "                best_loss=test_loss\n",
    "                torch.save(model.state_dict(),\"model_weight.pth\")\n",
    "\n",
    "            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\n",
    "        print(accuracy) \n",
    "\n",
    "\n",
    "test()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (out1): Linear(in_features=50176, out_features=224, bias=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (out2): Linear(in_features=224, out_features=50, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model_weight.pth'))\n",
    "torch.save(model,'model.pth')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([# Ensure the image is in grayscale if needed\n",
    "    transforms.Resize((224, 224)),  # Resize to the same size as the training images\n",
    "    transforms.ToTensor(), # Convert to tensor\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the test image\n",
    "image = Image.open('test_image.jpeg')\n",
    "image = transform(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: 3\n"
     ]
    }
   ],
   "source": [
    "# Classify the image\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    predicted_class = torch.argmax(output, dim=1)\n",
    "\n",
    "print(f'Predicted class: {predicted_class.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
